{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run-jump-start-rnn-sentiment-v002.py\n",
    "\n",
    "# Program by Thomas W. Miller, August 16, 2018\n",
    "\n",
    "# Previous work involved gathering embeddings via chakin\n",
    "# Following methods described in\n",
    "#    https://github.com/chakki-works/chakin\n",
    "# The previous program, run-chakin-to-get-embeddings-v001.py\n",
    "# downloaded pre-trained GloVe embeddings, saved them in a zip archive,\n",
    "# and unzipped that archive to create the four word-to-embeddings\n",
    "# text files for use in language models. \n",
    "\n",
    "# This program sets uses word embeddings to set up defaultdict \n",
    "# dictionary data structures, that can them be employed in language\n",
    "# models. This is demonstrated with a simple RNN model for predicting\n",
    "# sentiment (thumbs-down versus thumbs-up) for movie reviews.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 100000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 100)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477, -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]\n"
     ]
    }
   ],
   "source": [
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "quick:  [-0.43146   -0.22037   -0.22684   -0.10215   -0.31863   -0.11809\n",
      " -0.093402  -0.069789  -0.29029   -0.34006    0.099652  -0.059301\n",
      " -0.43764    0.19464    0.36997    0.73648   -0.53429   -0.3469\n",
      " -0.21415    0.62954    0.54868    0.29429   -0.32889   -0.61771\n",
      " -0.039648   0.91639   -0.64046    0.28725    0.095922  -0.38774\n",
      " -0.62958    0.33443   -0.4856    -0.2287     0.84277   -0.2204\n",
      " -0.13264   -0.18188    0.077686   0.080045  -0.018909  -0.26018\n",
      "  0.29542   -0.89173   -0.39373   -0.35662    0.011656  -0.37658\n",
      "  0.64576   -0.86503    0.12615    0.18984   -0.26936    0.56216\n",
      "  0.38218   -2.1389    -0.0096116  0.15041    1.2586    -0.35475\n",
      " -0.33285    0.07292   -0.077262   0.049068   0.90212   -0.27539\n",
      " -0.20839    0.26349   -0.26515   -0.70593   -0.68474    0.38424\n",
      " -0.21889   -0.88545    0.38583    0.26481   -0.7641    -0.037501\n",
      " -0.020606  -0.71318    1.1045     0.0453    -0.41902   -0.47667\n",
      " -1.4088    -0.50376    0.88062    0.0072194 -0.42083   -0.62586\n",
      "  0.59608    0.30444   -0.40999   -0.28204   -0.52321   -0.44695\n",
      "  0.21083   -0.010209   0.0086056  0.63263  ]\n",
      "brown:  [-4.3812e-01 -9.9389e-02 -2.6038e-01 -1.1084e+00  1.0550e-01 -5.4542e-02\n",
      "  4.4868e-01  6.1750e-02 -5.8803e-01 -2.1738e-01 -3.6304e-01 -4.0887e-01\n",
      "  3.7877e-02  8.4201e-01  1.0108e-01 -1.8530e-01  5.0486e-01 -3.4252e-01\n",
      "  2.2516e-01 -2.6942e-02 -4.6399e-01  9.9140e-02  1.9596e-02 -6.7435e-01\n",
      "  6.3123e-01  9.5930e-01  1.6215e-01 -4.3166e-01 -2.6642e-01  1.9136e-01\n",
      "  4.5626e-01  6.8918e-01  3.6808e-01 -2.8273e-01 -4.6525e-01  5.9984e-01\n",
      "  1.5369e-01  8.6585e-01  2.7917e-01  5.8380e-01 -4.6627e-01 -1.3590e+00\n",
      " -1.0387e-01  6.0146e-02 -5.2733e-01  1.3135e-01 -3.3766e-01  1.7893e-01\n",
      "  4.4812e-01 -7.0502e-01  6.3793e-01 -7.9508e-01  1.3176e-01  9.7769e-01\n",
      " -2.3153e-01 -2.6450e+00 -1.1464e-01  2.7907e-01  4.9121e-01  5.1274e-01\n",
      "  7.9559e-04  1.7932e-01 -2.9938e-01 -3.3465e-01  9.9161e-01 -6.0262e-01\n",
      "  7.2080e-01  8.4681e-01 -2.3669e-01  1.3666e-01 -3.5330e-01  3.9442e-01\n",
      " -7.2818e-01  9.1664e-02  3.0441e-01  4.8352e-02 -4.1140e-01  3.4362e-01\n",
      "  1.2569e-01  4.2484e-01  4.5470e-01  1.6292e-01 -1.3630e-01 -2.1827e-01\n",
      " -3.8261e-01 -9.2620e-01  5.1256e-01 -3.5184e-01  1.8316e-01  1.9807e-01\n",
      " -1.9681e-02 -7.2242e-01 -4.3439e-01  1.3449e-01 -8.4339e-01  1.3815e-02\n",
      " -1.1325e+00  1.8143e-01 -1.9537e-01 -3.6954e-01]\n",
      "fox:  [ 0.16917   -0.99783    0.24429   -0.79687    0.036447  -0.56127\n",
      "  0.17305    0.29287   -0.43291   -0.82274   -0.11437   -0.28808\n",
      "  0.20501   -0.4878     0.50534   -0.2117     0.48474    0.20959\n",
      "  0.26642    0.6839    -0.2629     0.14794    0.087969  -0.17349\n",
      "  0.61804    0.63733    0.41145    0.46401   -0.2165     0.5\n",
      "  0.65265    1.0608     0.19275    0.141      0.51356    0.72558\n",
      " -0.044848  -0.35761    0.49862    0.73592   -0.38307    0.12159\n",
      " -0.75345    0.80579   -0.48075   -0.40283   -0.49931   -0.60309\n",
      "  0.26126   -0.24109   -0.55885   -0.10622    0.11289    0.49708\n",
      "  0.015915  -2.452     -0.32529    0.20437    0.55361    0.60879\n",
      " -0.083061   0.60856    0.13958   -0.71847    1.1409     0.023752\n",
      "  0.050995   0.29621   -0.16247    1.1456     0.16929   -0.0042113\n",
      " -0.4026    -0.073144   0.096698  -0.15248   -0.69435    0.28032\n",
      " -1.0238     0.58777   -0.34573   -0.60871    0.1842    -0.18736\n",
      " -0.49948   -0.18095   -0.71161    0.69437    0.37298   -0.308\n",
      "  0.2455    -0.94515    0.20393   -0.14885   -1.1153    -0.52266\n",
      " -0.27841    0.027184   0.39712    0.17933  ]\n",
      "jumps:  [ 0.87831   0.76211   0.24562  -0.05516   0.10355  -0.6789   -0.36757\n",
      "  0.52207  -0.37174  -0.10266   1.0164    0.97297   0.028706  0.22013\n",
      "  0.36371   0.79072  -1.5199    0.72657   0.24994   0.07658   0.79373\n",
      "  0.32268  -0.28497   0.30724   0.25493   0.049801 -0.68182   0.059687\n",
      "  0.40362  -0.73308  -0.5968    0.2901    0.15876   0.070044  0.57204\n",
      "  0.70252  -0.86423  -0.1618   -0.026244  0.19154  -0.14515   0.34694\n",
      " -0.62756   0.15429  -0.56114   0.15854  -0.56041  -0.39705   0.31183\n",
      " -0.19028  -0.53601   0.061462  0.12484   1.3302    0.34361  -1.1603\n",
      "  0.10341   0.33138   0.74712   0.11517   0.17949   0.059578  0.22881\n",
      "  0.52396  -0.43749   0.33677   0.028801 -0.67852   0.21443   0.038026\n",
      " -0.87474  -0.22532   0.020465  1.0772    0.71369  -0.14903  -0.53563\n",
      " -0.049547  0.23989  -0.19058   0.13683   0.29553  -0.20244  -0.40515\n",
      " -0.24246  -1.0324    0.32728  -0.46241   0.27757  -0.23512  -0.23432\n",
      "  0.1031   -0.54905   0.21484  -0.16597  -0.34962  -0.16015  -0.2617\n",
      "  0.41802  -0.055161]\n",
      "over:  [-2.9574e-01  3.5345e-01  6.3326e-01  1.9576e-01 -3.0256e-02  5.4244e-01\n",
      " -2.1091e-01  3.2894e-01 -4.8888e-01  1.8379e-01  2.4242e-01  4.0346e-01\n",
      "  1.1973e-01  1.3143e-02  2.4154e-01 -4.0184e-01  2.2176e-01 -2.7837e-01\n",
      " -4.6930e-01 -5.4899e-02  6.5148e-01  1.5958e-01  5.9556e-01  3.3167e-01\n",
      "  7.2649e-01 -4.3182e-01  1.7208e-01 -1.1584e-02 -2.6389e-01 -2.2073e-01\n",
      " -2.8538e-01  3.5863e-01  2.4592e-01  2.2143e-01 -7.6221e-01  3.9352e-01\n",
      " -2.3915e-02  4.3028e-01 -4.7099e-01  2.5162e-01 -5.9507e-01 -1.0495e+00\n",
      "  1.7973e-01 -3.1621e-01  2.3788e-01 -8.8560e-02  3.4751e-01 -5.5950e-01\n",
      "  1.2997e-01 -7.0101e-01  2.8850e-01  1.8111e-01 -2.3004e-01  2.0682e+00\n",
      " -1.4925e-01 -2.8700e+00 -4.6722e-03 -2.2819e-01  1.6623e+00  6.5951e-01\n",
      "  2.1892e-01  6.3600e-01  1.0332e-01  1.3176e-03  4.4414e-01  2.0222e-01\n",
      "  5.2490e-01  6.4131e-01  2.7416e-01  1.0695e-01 -1.2030e-01  4.7109e-02\n",
      " -5.3503e-01 -4.6869e-01 -7.6050e-02  1.0654e-03 -3.8456e-01 -2.4067e-02\n",
      " -7.5877e-01  5.2622e-01  1.3285e+00 -3.9051e-01 -1.2174e-01  5.1886e-01\n",
      " -1.0374e+00 -3.3789e-01  7.4933e-02  2.0036e-01  2.4703e-02 -2.9090e-01\n",
      " -3.2043e-01  2.0445e-02 -9.9185e-01  1.6802e-02 -6.0819e-01 -2.6601e-01\n",
      " -1.9549e-01  2.3127e-01  9.4771e-01 -9.5560e-02]\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "lazy:  [ 0.14481   -0.20397    0.3596    -0.59938   -0.93979    0.59784\n",
      " -0.21619    0.73051   -0.36588   -0.19962    0.14571    0.1642\n",
      "  0.1086    -0.78575    0.53327    0.37127   -0.33013   -0.082276\n",
      "  0.73923    0.86931    0.37934    1.2427    -0.19554   -0.53849\n",
      "  0.20681    0.76727   -0.9714    -0.016255  -0.12529    0.36231\n",
      "  0.13313    0.60993    0.44345   -0.3654     0.22531    0.72985\n",
      " -0.69992    0.14427    0.85324    0.21268   -0.46674    0.25746\n",
      " -0.88493    0.042164  -0.24125   -0.11241   -0.52837    0.38905\n",
      "  0.35523    0.29078   -0.47363   -0.30561    0.072255   0.31778\n",
      " -0.64297   -0.3527     0.49651    0.29722    0.68888   -0.54184\n",
      "  0.04863    0.26221   -0.61438   -0.2591     0.66305    0.25526\n",
      "  0.42406   -0.22196   -0.053041  -0.80721   -0.89748   -0.1165\n",
      "  0.45258    0.24817   -0.14874   -0.20952   -0.58499    0.5573\n",
      "  0.47503   -0.6429    -0.11219    0.2627    -0.4951    -0.0085495\n",
      " -0.86135   -0.21422    0.0086754  0.35554   -0.48077   -0.39897\n",
      " -0.012746   0.13761   -0.20283    0.40565    0.056275  -0.35009\n",
      " -0.745     -0.42987   -0.56238   -0.13433  ]\n",
      "dog:  [ 0.30817    0.30938    0.52803   -0.92543   -0.73671    0.63475\n",
      "  0.44197    0.10262   -0.09142   -0.56607   -0.5327     0.2013\n",
      "  0.7704    -0.13983    0.13727    1.1128     0.89301   -0.17869\n",
      " -0.0019722  0.57289    0.59479    0.50428   -0.28991   -1.3491\n",
      "  0.42756    1.2748    -1.1613    -0.41084    0.042804   0.54866\n",
      "  0.18897    0.3759     0.58035    0.66975    0.81156    0.93864\n",
      " -0.51005   -0.070079   0.82819   -0.35346    0.21086   -0.24412\n",
      " -0.16554   -0.78358   -0.48482    0.38968   -0.86356   -0.016391\n",
      "  0.31984   -0.49246   -0.069363   0.018869  -0.098286   1.3126\n",
      " -0.12116   -1.2399    -0.091429   0.35294    0.64645    0.089642\n",
      "  0.70294    1.1244     0.38639    0.52084    0.98787    0.79952\n",
      " -0.34625    0.14095    0.80167    0.20987   -0.86007   -0.15308\n",
      "  0.074523   0.40816    0.019208   0.51587   -0.34428   -0.24525\n",
      " -0.77984    0.27425    0.22418    0.20164    0.017431  -0.014697\n",
      " -1.0235    -0.39695   -0.0056188  0.30569    0.31748    0.021404\n",
      "  0.11837   -0.11319    0.42456    0.53405   -0.16717   -0.27185\n",
      " -0.6255     0.12883    0.62529   -0.52086  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 100000 words:\n",
      "\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "quick:  [-0.43146   -0.22037   -0.22684   -0.10215   -0.31863   -0.11809\n",
      " -0.093402  -0.069789  -0.29029   -0.34006    0.099652  -0.059301\n",
      " -0.43764    0.19464    0.36997    0.73648   -0.53429   -0.3469\n",
      " -0.21415    0.62954    0.54868    0.29429   -0.32889   -0.61771\n",
      " -0.039648   0.91639   -0.64046    0.28725    0.095922  -0.38774\n",
      " -0.62958    0.33443   -0.4856    -0.2287     0.84277   -0.2204\n",
      " -0.13264   -0.18188    0.077686   0.080045  -0.018909  -0.26018\n",
      "  0.29542   -0.89173   -0.39373   -0.35662    0.011656  -0.37658\n",
      "  0.64576   -0.86503    0.12615    0.18984   -0.26936    0.56216\n",
      "  0.38218   -2.1389    -0.0096116  0.15041    1.2586    -0.35475\n",
      " -0.33285    0.07292   -0.077262   0.049068   0.90212   -0.27539\n",
      " -0.20839    0.26349   -0.26515   -0.70593   -0.68474    0.38424\n",
      " -0.21889   -0.88545    0.38583    0.26481   -0.7641    -0.037501\n",
      " -0.020606  -0.71318    1.1045     0.0453    -0.41902   -0.47667\n",
      " -1.4088    -0.50376    0.88062    0.0072194 -0.42083   -0.62586\n",
      "  0.59608    0.30444   -0.40999   -0.28204   -0.52321   -0.44695\n",
      "  0.21083   -0.010209   0.0086056  0.63263  ]\n",
      "brown:  [-4.3812e-01 -9.9389e-02 -2.6038e-01 -1.1084e+00  1.0550e-01 -5.4542e-02\n",
      "  4.4868e-01  6.1750e-02 -5.8803e-01 -2.1738e-01 -3.6304e-01 -4.0887e-01\n",
      "  3.7877e-02  8.4201e-01  1.0108e-01 -1.8530e-01  5.0486e-01 -3.4252e-01\n",
      "  2.2516e-01 -2.6942e-02 -4.6399e-01  9.9140e-02  1.9596e-02 -6.7435e-01\n",
      "  6.3123e-01  9.5930e-01  1.6215e-01 -4.3166e-01 -2.6642e-01  1.9136e-01\n",
      "  4.5626e-01  6.8918e-01  3.6808e-01 -2.8273e-01 -4.6525e-01  5.9984e-01\n",
      "  1.5369e-01  8.6585e-01  2.7917e-01  5.8380e-01 -4.6627e-01 -1.3590e+00\n",
      " -1.0387e-01  6.0146e-02 -5.2733e-01  1.3135e-01 -3.3766e-01  1.7893e-01\n",
      "  4.4812e-01 -7.0502e-01  6.3793e-01 -7.9508e-01  1.3176e-01  9.7769e-01\n",
      " -2.3153e-01 -2.6450e+00 -1.1464e-01  2.7907e-01  4.9121e-01  5.1274e-01\n",
      "  7.9559e-04  1.7932e-01 -2.9938e-01 -3.3465e-01  9.9161e-01 -6.0262e-01\n",
      "  7.2080e-01  8.4681e-01 -2.3669e-01  1.3666e-01 -3.5330e-01  3.9442e-01\n",
      " -7.2818e-01  9.1664e-02  3.0441e-01  4.8352e-02 -4.1140e-01  3.4362e-01\n",
      "  1.2569e-01  4.2484e-01  4.5470e-01  1.6292e-01 -1.3630e-01 -2.1827e-01\n",
      " -3.8261e-01 -9.2620e-01  5.1256e-01 -3.5184e-01  1.8316e-01  1.9807e-01\n",
      " -1.9681e-02 -7.2242e-01 -4.3439e-01  1.3449e-01 -8.4339e-01  1.3815e-02\n",
      " -1.1325e+00  1.8143e-01 -1.9537e-01 -3.6954e-01]\n",
      "fox:  [ 0.16917   -0.99783    0.24429   -0.79687    0.036447  -0.56127\n",
      "  0.17305    0.29287   -0.43291   -0.82274   -0.11437   -0.28808\n",
      "  0.20501   -0.4878     0.50534   -0.2117     0.48474    0.20959\n",
      "  0.26642    0.6839    -0.2629     0.14794    0.087969  -0.17349\n",
      "  0.61804    0.63733    0.41145    0.46401   -0.2165     0.5\n",
      "  0.65265    1.0608     0.19275    0.141      0.51356    0.72558\n",
      " -0.044848  -0.35761    0.49862    0.73592   -0.38307    0.12159\n",
      " -0.75345    0.80579   -0.48075   -0.40283   -0.49931   -0.60309\n",
      "  0.26126   -0.24109   -0.55885   -0.10622    0.11289    0.49708\n",
      "  0.015915  -2.452     -0.32529    0.20437    0.55361    0.60879\n",
      " -0.083061   0.60856    0.13958   -0.71847    1.1409     0.023752\n",
      "  0.050995   0.29621   -0.16247    1.1456     0.16929   -0.0042113\n",
      " -0.4026    -0.073144   0.096698  -0.15248   -0.69435    0.28032\n",
      " -1.0238     0.58777   -0.34573   -0.60871    0.1842    -0.18736\n",
      " -0.49948   -0.18095   -0.71161    0.69437    0.37298   -0.308\n",
      "  0.2455    -0.94515    0.20393   -0.14885   -1.1153    -0.52266\n",
      " -0.27841    0.027184   0.39712    0.17933  ]\n",
      "jumps:  [ 0.87831   0.76211   0.24562  -0.05516   0.10355  -0.6789   -0.36757\n",
      "  0.52207  -0.37174  -0.10266   1.0164    0.97297   0.028706  0.22013\n",
      "  0.36371   0.79072  -1.5199    0.72657   0.24994   0.07658   0.79373\n",
      "  0.32268  -0.28497   0.30724   0.25493   0.049801 -0.68182   0.059687\n",
      "  0.40362  -0.73308  -0.5968    0.2901    0.15876   0.070044  0.57204\n",
      "  0.70252  -0.86423  -0.1618   -0.026244  0.19154  -0.14515   0.34694\n",
      " -0.62756   0.15429  -0.56114   0.15854  -0.56041  -0.39705   0.31183\n",
      " -0.19028  -0.53601   0.061462  0.12484   1.3302    0.34361  -1.1603\n",
      "  0.10341   0.33138   0.74712   0.11517   0.17949   0.059578  0.22881\n",
      "  0.52396  -0.43749   0.33677   0.028801 -0.67852   0.21443   0.038026\n",
      " -0.87474  -0.22532   0.020465  1.0772    0.71369  -0.14903  -0.53563\n",
      " -0.049547  0.23989  -0.19058   0.13683   0.29553  -0.20244  -0.40515\n",
      " -0.24246  -1.0324    0.32728  -0.46241   0.27757  -0.23512  -0.23432\n",
      "  0.1031   -0.54905   0.21484  -0.16597  -0.34962  -0.16015  -0.2617\n",
      "  0.41802  -0.055161]\n",
      "over:  [-2.9574e-01  3.5345e-01  6.3326e-01  1.9576e-01 -3.0256e-02  5.4244e-01\n",
      " -2.1091e-01  3.2894e-01 -4.8888e-01  1.8379e-01  2.4242e-01  4.0346e-01\n",
      "  1.1973e-01  1.3143e-02  2.4154e-01 -4.0184e-01  2.2176e-01 -2.7837e-01\n",
      " -4.6930e-01 -5.4899e-02  6.5148e-01  1.5958e-01  5.9556e-01  3.3167e-01\n",
      "  7.2649e-01 -4.3182e-01  1.7208e-01 -1.1584e-02 -2.6389e-01 -2.2073e-01\n",
      " -2.8538e-01  3.5863e-01  2.4592e-01  2.2143e-01 -7.6221e-01  3.9352e-01\n",
      " -2.3915e-02  4.3028e-01 -4.7099e-01  2.5162e-01 -5.9507e-01 -1.0495e+00\n",
      "  1.7973e-01 -3.1621e-01  2.3788e-01 -8.8560e-02  3.4751e-01 -5.5950e-01\n",
      "  1.2997e-01 -7.0101e-01  2.8850e-01  1.8111e-01 -2.3004e-01  2.0682e+00\n",
      " -1.4925e-01 -2.8700e+00 -4.6722e-03 -2.2819e-01  1.6623e+00  6.5951e-01\n",
      "  2.1892e-01  6.3600e-01  1.0332e-01  1.3176e-03  4.4414e-01  2.0222e-01\n",
      "  5.2490e-01  6.4131e-01  2.7416e-01  1.0695e-01 -1.2030e-01  4.7109e-02\n",
      " -5.3503e-01 -4.6869e-01 -7.6050e-02  1.0654e-03 -3.8456e-01 -2.4067e-02\n",
      " -7.5877e-01  5.2622e-01  1.3285e+00 -3.9051e-01 -1.2174e-01  5.1886e-01\n",
      " -1.0374e+00 -3.3789e-01  7.4933e-02  2.0036e-01  2.4703e-02 -2.9090e-01\n",
      " -3.2043e-01  2.0445e-02 -9.9185e-01  1.6802e-02 -6.0819e-01 -2.6601e-01\n",
      " -1.9549e-01  2.3127e-01  9.4771e-01 -9.5560e-02]\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "lazy:  [ 0.14481   -0.20397    0.3596    -0.59938   -0.93979    0.59784\n",
      " -0.21619    0.73051   -0.36588   -0.19962    0.14571    0.1642\n",
      "  0.1086    -0.78575    0.53327    0.37127   -0.33013   -0.082276\n",
      "  0.73923    0.86931    0.37934    1.2427    -0.19554   -0.53849\n",
      "  0.20681    0.76727   -0.9714    -0.016255  -0.12529    0.36231\n",
      "  0.13313    0.60993    0.44345   -0.3654     0.22531    0.72985\n",
      " -0.69992    0.14427    0.85324    0.21268   -0.46674    0.25746\n",
      " -0.88493    0.042164  -0.24125   -0.11241   -0.52837    0.38905\n",
      "  0.35523    0.29078   -0.47363   -0.30561    0.072255   0.31778\n",
      " -0.64297   -0.3527     0.49651    0.29722    0.68888   -0.54184\n",
      "  0.04863    0.26221   -0.61438   -0.2591     0.66305    0.25526\n",
      "  0.42406   -0.22196   -0.053041  -0.80721   -0.89748   -0.1165\n",
      "  0.45258    0.24817   -0.14874   -0.20952   -0.58499    0.5573\n",
      "  0.47503   -0.6429    -0.11219    0.2627    -0.4951    -0.0085495\n",
      " -0.86135   -0.21422    0.0086754  0.35554   -0.48077   -0.39897\n",
      " -0.012746   0.13761   -0.20283    0.40565    0.056275  -0.35009\n",
      " -0.745     -0.42987   -0.56238   -0.13433  ]\n",
      "dog:  [ 0.30817    0.30938    0.52803   -0.92543   -0.73671    0.63475\n",
      "  0.44197    0.10262   -0.09142   -0.56607   -0.5327     0.2013\n",
      "  0.7704    -0.13983    0.13727    1.1128     0.89301   -0.17869\n",
      " -0.0019722  0.57289    0.59479    0.50428   -0.28991   -1.3491\n",
      "  0.42756    1.2748    -1.1613    -0.41084    0.042804   0.54866\n",
      "  0.18897    0.3759     0.58035    0.66975    0.81156    0.93864\n",
      " -0.51005   -0.070079   0.82819   -0.35346    0.21086   -0.24412\n",
      " -0.16554   -0.78358   -0.48482    0.38968   -0.86356   -0.016391\n",
      "  0.31984   -0.49246   -0.069363   0.018869  -0.098286   1.3126\n",
      " -0.12116   -1.2399    -0.091429   0.35294    0.64645    0.089642\n",
      "  0.70294    1.1244     0.38639    0.52084    0.98787    0.79952\n",
      " -0.34625    0.14095    0.80167    0.20987   -0.86007   -0.15308\n",
      "  0.074523   0.40816    0.019208   0.51587   -0.34428   -0.24525\n",
      " -0.77984    0.27425    0.22418    0.20164    0.017431  -0.014697\n",
      " -1.0235    -0.39695   -0.0056188  0.30569    0.31748    0.021404\n",
      "  0.11837   -0.11319    0.42456    0.53405   -0.16717   -0.27185\n",
      " -0.6255     0.12883    0.62529   -0.52086  ]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "    #I guess the tokenizer outputs a list of strings, not an array or tokenizer oject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24487"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember you have a embedding matrix prepared with limited words of 10,000\n",
    "limited_index_to_embedding[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = [] \n",
    "sequences = []\n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    sequence = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]])\n",
    "       sequence.append(limited_word_to_index[word]) \n",
    "    embeddings.append(embedding)\n",
    "    sequences.append(sequence)\n",
    "#embeddings has three dimensions\n",
    "#sequences has two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[523,\n",
       " 3,\n",
       " 300,\n",
       " 38,\n",
       " 31,\n",
       " 28638,\n",
       " 5044,\n",
       " 10,\n",
       " 9610,\n",
       " 2383,\n",
       " 66,\n",
       " 17,\n",
       " 875,\n",
       " 1500,\n",
       " 12,\n",
       " 14,\n",
       " 11025,\n",
       " 880,\n",
       " 3,\n",
       " 12413,\n",
       " 17,\n",
       " 77,\n",
       " 219,\n",
       " 22181,\n",
       " 21,\n",
       " 581,\n",
       " 353,\n",
       " 100000,\n",
       " 100000,\n",
       " 581,\n",
       " 1569,\n",
       " 10945,\n",
       " 23760,\n",
       " 5,\n",
       " 15678,\n",
       " 16083,\n",
       " 86,\n",
       " 30,\n",
       " 541,\n",
       " 3442]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 40)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "sequences_array =np.array(sequences)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "## up to this point its all been about the embedding layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)\n",
    "#I ADJUST THE TRAIN_TEST_SPLIT FROM embeddings_array to sequences_array\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(sequences_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "#X_train is a list of 40 sequences that have vectors associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is where I will start implementing my own KERAS code\n",
    "## i need my X_train data to be in the right shape and form\n",
    "##i need my Embedding Matrix to be set up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limited_index_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Embedding(EVOCABSIZE+1,#num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "                           100,\n",
    "                           embeddings_initializer = keras.initializers.Constant(limited_index_to_embedding),\n",
    "                           input_length = 40,\n",
    "                           trainable=False)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 100)           10000100  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,251,109\n",
      "Trainable params: 251,009\n",
      "Non-trainable params: 10,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(128)))\n",
    "model.add(keras.layers.Dense(64,activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer = keras.optimizers.Adam(.0001),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#figuring out what the inputs look like for this tf model\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "# dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "#                           as_supervised=True)\n",
    "# train_dataset, test_dataset = dataset['train'], dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3410 - accuracy: 0.8487 - val_loss: 0.4943 - val_accuracy: 0.7350\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3335 - accuracy: 0.8487 - val_loss: 0.5034 - val_accuracy: 0.7650\n",
      "Epoch 3/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3188 - accuracy: 0.8675 - val_loss: 0.5044 - val_accuracy: 0.7550\n",
      "Epoch 4/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3198 - accuracy: 0.8587 - val_loss: 0.5149 - val_accuracy: 0.7750\n",
      "Epoch 5/30\n",
      "800/800 [==============================] - 1s 2ms/sample - loss: 0.3168 - accuracy: 0.8562 - val_loss: 0.5049 - val_accuracy: 0.7450\n",
      "Epoch 6/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3135 - accuracy: 0.8612 - val_loss: 0.5453 - val_accuracy: 0.7850\n",
      "Epoch 7/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.3029 - accuracy: 0.8662 - val_loss: 0.5309 - val_accuracy: 0.7300\n",
      "Epoch 8/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2901 - accuracy: 0.8700 - val_loss: 0.5441 - val_accuracy: 0.7800\n",
      "Epoch 9/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2867 - accuracy: 0.8775 - val_loss: 0.5126 - val_accuracy: 0.7400\n",
      "Epoch 10/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2498 - accuracy: 0.9038 - val_loss: 0.5288 - val_accuracy: 0.7600\n",
      "Epoch 11/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2454 - accuracy: 0.9038 - val_loss: 0.5262 - val_accuracy: 0.7350\n",
      "Epoch 12/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2296 - accuracy: 0.9125 - val_loss: 0.5452 - val_accuracy: 0.7600\n",
      "Epoch 13/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2323 - accuracy: 0.8900 - val_loss: 0.5863 - val_accuracy: 0.7550\n",
      "Epoch 14/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2203 - accuracy: 0.9175 - val_loss: 0.5708 - val_accuracy: 0.7200\n",
      "Epoch 15/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1999 - accuracy: 0.9275 - val_loss: 0.6116 - val_accuracy: 0.7350\n",
      "Epoch 16/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2015 - accuracy: 0.9200 - val_loss: 0.5947 - val_accuracy: 0.7150\n",
      "Epoch 17/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1943 - accuracy: 0.9112 - val_loss: 0.6427 - val_accuracy: 0.7250\n",
      "Epoch 18/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.2016 - accuracy: 0.9087 - val_loss: 0.6635 - val_accuracy: 0.7350\n",
      "Epoch 19/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1810 - accuracy: 0.9362 - val_loss: 0.6465 - val_accuracy: 0.7250\n",
      "Epoch 20/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1461 - accuracy: 0.9488 - val_loss: 0.6485 - val_accuracy: 0.7300\n",
      "Epoch 21/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1351 - accuracy: 0.9600 - val_loss: 0.6869 - val_accuracy: 0.7200\n",
      "Epoch 22/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1286 - accuracy: 0.9575 - val_loss: 0.7127 - val_accuracy: 0.7150\n",
      "Epoch 23/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1202 - accuracy: 0.9600 - val_loss: 0.7720 - val_accuracy: 0.7200\n",
      "Epoch 24/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1065 - accuracy: 0.9700 - val_loss: 0.7530 - val_accuracy: 0.7100\n",
      "Epoch 25/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.0976 - accuracy: 0.9700 - val_loss: 0.8180 - val_accuracy: 0.7100\n",
      "Epoch 26/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1054 - accuracy: 0.9663 - val_loss: 0.8207 - val_accuracy: 0.7050\n",
      "Epoch 27/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.1074 - accuracy: 0.9600 - val_loss: 0.8253 - val_accuracy: 0.7150\n",
      "Epoch 28/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.0840 - accuracy: 0.9775 - val_loss: 0.8378 - val_accuracy: 0.7400\n",
      "Epoch 29/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.0642 - accuracy: 0.9900 - val_loss: 0.9080 - val_accuracy: 0.7050\n",
      "Epoch 30/30\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 0.0601 - accuracy: 0.9925 - val_loss: 0.8892 - val_accuracy: 0.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2294fc90fc8>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,\n",
    "         batch_size = 64,\n",
    "         epochs = 30,\n",
    "         validation_data = (X_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# all code from here and down is scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = keras.layers.Input(shape = (40,),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think this will end up looking like the embeddings 3d array from class code\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 5 from 3 for 'max_pooling1d_1/MaxPool' (op: 'MaxPool') with input shapes: [?,3,1,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 5 from 3 for 'max_pooling1d_1/MaxPool' (op: 'MaxPool') with input shapes: [?,3,1,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-ae7f9687f784>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    772\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\pooling.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         data_format=self.data_format)\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[1;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[0;32m   5288\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'max'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5289\u001b[0m     x = nn.max_pool(\n\u001b[1;32m-> 5290\u001b[1;33m         x, pool_size, strides, padding=padding, data_format=tf_data_format)\n\u001b[0m\u001b[0;32m   5291\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'avg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5292\u001b[0m     x = nn.avg_pool(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[1;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[0;32m   3873\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3874\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3875\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   3876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[1;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[0;32m   5197\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m   5198\u001b[0m         \u001b[1;34m\"MaxPool\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5199\u001b[1;33m                    data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m   5200\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5201\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 5 from 3 for 'max_pooling1d_1/MaxPool' (op: 'MaxPool') with input shapes: [?,3,1,128]."
     ]
    }
   ],
   "source": [
    "x = keras.layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = keras.layers.MaxPooling1D(5)(x)\n",
    "x = keras.layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = keras.layers.MaxPooling1D(5)(x)\n",
    "x = keras.layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dense(0, activation='relu')(x)\n",
    "preds = keras.layers.Dense(1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: story\n",
      "Embedding for this word:\n",
      " [-1.6987e-01  7.4241e-01  4.3299e-01 -4.5484e-01  9.9715e-02  7.1426e-01\n",
      " -6.4353e-02 -2.5535e-01  2.5013e-01 -2.9634e-01  7.7283e-01  5.5974e-01\n",
      " -6.2684e-02  2.1245e-01 -1.1726e-01  6.0059e-01  2.9025e-01  5.7505e-02\n",
      " -1.5385e-01  3.2536e-01  5.8805e-01 -3.9970e-01 -3.6610e-01  2.6827e-01\n",
      "  1.0531e+00 -2.2903e-01 -2.3219e-02 -3.0292e-01 -6.6937e-01  3.6806e-01\n",
      " -4.5865e-02  2.6706e-01 -1.0901e-01 -4.4617e-01 -5.7160e-01 -5.3780e-02\n",
      " -5.4940e-02  8.8485e-03  5.9055e-01 -8.7651e-02 -2.4036e-01  1.7260e-01\n",
      " -5.4645e-01  2.2699e-02  8.6777e-02 -1.6234e-02 -4.4207e-01 -4.9677e-01\n",
      "  6.5760e-01 -1.9751e-01  2.5612e-01  3.6991e-02  1.0723e+00  1.3145e+00\n",
      " -2.9942e-02 -2.7365e+00 -1.5057e-01  2.7365e-01  1.0993e+00  4.0902e-02\n",
      " -4.8319e-01  1.6665e+00 -4.6939e-02 -7.2553e-01  1.4452e+00 -6.4141e-01\n",
      "  6.4354e-01  4.6000e-01 -4.4806e-01  3.6381e-01  2.3893e-03  3.2208e-01\n",
      " -2.6913e-01 -1.1554e-01  5.3460e-01  3.1170e-01  3.5561e-01 -1.8954e-01\n",
      " -1.0408e+00 -7.2981e-02 -1.0390e+00  5.1150e-01  1.9974e-01 -2.0269e-01\n",
      " -1.0895e+00 -1.5243e-01 -7.6189e-01 -7.9637e-01 -2.6106e-01 -2.8558e-01\n",
      "  4.4818e-01  9.7420e-02  2.9502e-01 -1.5510e-01 -1.2594e-01 -1.9690e-01\n",
      " -7.5945e-02 -9.6738e-01  1.3695e-01  7.2302e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-1.6987e-01  7.4241e-01  4.3299e-01 -4.5484e-01  9.9715e-02  7.1426e-01\n",
      " -6.4353e-02 -2.5535e-01  2.5013e-01 -2.9634e-01  7.7283e-01  5.5974e-01\n",
      " -6.2684e-02  2.1245e-01 -1.1726e-01  6.0059e-01  2.9025e-01  5.7505e-02\n",
      " -1.5385e-01  3.2536e-01  5.8805e-01 -3.9970e-01 -3.6610e-01  2.6827e-01\n",
      "  1.0531e+00 -2.2903e-01 -2.3219e-02 -3.0292e-01 -6.6937e-01  3.6806e-01\n",
      " -4.5865e-02  2.6706e-01 -1.0901e-01 -4.4617e-01 -5.7160e-01 -5.3780e-02\n",
      " -5.4940e-02  8.8485e-03  5.9055e-01 -8.7651e-02 -2.4036e-01  1.7260e-01\n",
      " -5.4645e-01  2.2699e-02  8.6777e-02 -1.6234e-02 -4.4207e-01 -4.9677e-01\n",
      "  6.5760e-01 -1.9751e-01  2.5612e-01  3.6991e-02  1.0723e+00  1.3145e+00\n",
      " -2.9942e-02 -2.7365e+00 -1.5057e-01  2.7365e-01  1.0993e+00  4.0902e-02\n",
      " -4.8319e-01  1.6665e+00 -4.6939e-02 -7.2553e-01  1.4452e+00 -6.4141e-01\n",
      "  6.4354e-01  4.6000e-01 -4.4806e-01  3.6381e-01  2.3893e-03  3.2208e-01\n",
      " -2.6913e-01 -1.1554e-01  5.3460e-01  3.1170e-01  3.5561e-01 -1.8954e-01\n",
      " -1.0408e+00 -7.2981e-02 -1.0390e+00  5.1150e-01  1.9974e-01 -2.0269e-01\n",
      " -1.0895e+00 -1.5243e-01 -7.6189e-01 -7.9637e-01 -2.6106e-01 -2.8558e-01\n",
      "  4.4818e-01  9.7420e-02  2.9502e-01 -1.5510e-01 -1.2594e-01 -1.9690e-01\n",
      " -7.5945e-02 -9.6738e-01  1.3695e-01  7.2302e-01]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: but\n",
      "Embedding for this word:\n",
      " [-5.7078e-02  3.9874e-01  6.8861e-01 -6.8151e-01 -4.5583e-01  2.0080e-01\n",
      "  1.7974e-01  5.3648e-02  4.3762e-01 -2.6725e-02  1.3383e-01 -7.8137e-03\n",
      "  4.2207e-01 -3.1801e-01  1.8065e-01 -3.5387e-01 -3.0929e-01  4.0660e-02\n",
      " -4.8854e-01  3.7910e-01  4.7955e-01 -4.1942e-02  4.0894e-01  1.2419e-01\n",
      "  4.0096e-01  1.9545e-01 -3.7819e-01 -7.7684e-01 -2.0677e-01 -4.3130e-01\n",
      " -1.0095e-01  3.9866e-01 -2.9612e-01 -8.3111e-02 -1.9026e-02  5.3927e-01\n",
      "  1.1912e-03  3.0235e-01 -3.6048e-01 -4.8434e-01 -4.7751e-01 -3.3922e-01\n",
      "  3.4788e-01 -1.7484e-01 -2.2613e-01 -3.2910e-01  8.1259e-01 -5.8452e-01\n",
      "  1.4509e-01 -7.1497e-01  1.7107e-01 -2.4833e-01  2.2104e-01  1.5517e+00\n",
      "  4.0869e-02 -2.9103e+00 -2.0812e-01 -1.7625e-01  1.6597e+00  8.6277e-01\n",
      " -3.2527e-01  6.5641e-01 -1.3142e-01  3.2312e-01  9.0836e-01 -2.9105e-01\n",
      "  8.4975e-01  5.3217e-01  1.5041e-01 -2.7983e-01 -2.9015e-02 -6.3378e-01\n",
      "  1.2237e-01 -7.9144e-01  1.6108e-01  1.7446e-02 -3.5095e-01 -1.6949e-01\n",
      " -1.0001e+00 -3.6832e-02  8.1140e-01 -2.2710e-01 -6.2133e-01  1.6484e-01\n",
      " -1.6804e+00 -3.9861e-01  6.3602e-02  1.0644e-01 -5.7955e-01 -4.5573e-01\n",
      " -3.7633e-02 -6.3445e-01 -3.0094e-01  3.9828e-01 -8.2883e-01  3.3827e-01\n",
      " -2.3613e-01 -1.9357e-01 -3.0606e-02  2.3970e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-5.7078e-02  3.9874e-01  6.8861e-01 -6.8151e-01 -4.5583e-01  2.0080e-01\n",
      "  1.7974e-01  5.3648e-02  4.3762e-01 -2.6725e-02  1.3383e-01 -7.8137e-03\n",
      "  4.2207e-01 -3.1801e-01  1.8065e-01 -3.5387e-01 -3.0929e-01  4.0660e-02\n",
      " -4.8854e-01  3.7910e-01  4.7955e-01 -4.1942e-02  4.0894e-01  1.2419e-01\n",
      "  4.0096e-01  1.9545e-01 -3.7819e-01 -7.7684e-01 -2.0677e-01 -4.3130e-01\n",
      " -1.0095e-01  3.9866e-01 -2.9612e-01 -8.3111e-02 -1.9026e-02  5.3927e-01\n",
      "  1.1912e-03  3.0235e-01 -3.6048e-01 -4.8434e-01 -4.7751e-01 -3.3922e-01\n",
      "  3.4788e-01 -1.7484e-01 -2.2613e-01 -3.2910e-01  8.1259e-01 -5.8452e-01\n",
      "  1.4509e-01 -7.1497e-01  1.7107e-01 -2.4833e-01  2.2104e-01  1.5517e+00\n",
      "  4.0869e-02 -2.9103e+00 -2.0812e-01 -1.7625e-01  1.6597e+00  8.6277e-01\n",
      " -3.2527e-01  6.5641e-01 -1.3142e-01  3.2312e-01  9.0836e-01 -2.9105e-01\n",
      "  8.4975e-01  5.3217e-01  1.5041e-01 -2.7983e-01 -2.9015e-02 -6.3378e-01\n",
      "  1.2237e-01 -7.9144e-01  1.6108e-01  1.7446e-02 -3.5095e-01 -1.6949e-01\n",
      " -1.0001e+00 -3.6832e-02  8.1140e-01 -2.2710e-01 -6.2133e-01  1.6484e-01\n",
      " -1.6804e+00 -3.9861e-01  6.3602e-02  1.0644e-01 -5.7955e-01 -4.5573e-01\n",
      " -3.7633e-02 -6.3445e-01 -3.0094e-01  3.9828e-01 -8.2883e-01  3.3827e-01\n",
      " -2.3613e-01 -1.9357e-01 -3.0606e-02  2.3970e-01]\n"
     ]
    }
   ],
   "source": [
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: from\n",
      "Embedding for this word:\n",
      " [ 3.0731e-01  2.4737e-01  6.8231e-01 -5.2367e-01  4.4053e-01  4.2044e-01\n",
      "  2.5140e-04  1.5265e-01 -6.1363e-01  2.2631e-01  8.3071e-02  7.0425e-02\n",
      "  1.7683e-02  5.6807e-01  1.0067e+00 -4.6206e-01  4.4524e-01 -5.0984e-01\n",
      " -4.2985e-01  1.9935e-01  2.2729e-01  5.1662e-01  5.6282e-01  4.1282e-01\n",
      "  1.7742e-01 -1.5694e-01 -1.1505e-01 -3.8050e-01  4.7440e-01 -1.6686e-01\n",
      "  2.3153e-01  6.3698e-02 -1.0716e-01 -2.6848e-01 -4.2665e-01  5.2237e-01\n",
      "  9.5376e-02  6.4020e-01 -5.2221e-01 -1.3856e-01 -9.8307e-01 -3.5320e-01\n",
      " -5.2161e-01  1.1277e-01  3.1634e-01  1.3297e-01 -4.9571e-02 -1.3785e-01\n",
      "  1.1317e-01 -5.0644e-01  3.8373e-01  3.6698e-01  3.9106e-01  9.8143e-01\n",
      " -5.4410e-01 -2.4640e+00 -6.8383e-01 -9.6243e-01  2.2017e+00  5.6643e-01\n",
      " -4.9410e-02  1.3093e+00 -4.0073e-01  8.3530e-01  1.7440e-01  4.4926e-02\n",
      "  5.4118e-01 -1.1038e-01  3.8200e-01  1.5369e-01 -3.7072e-01 -1.3141e-01\n",
      " -5.2504e-01 -5.6775e-01 -1.6822e-01 -9.1726e-02  8.1418e-02  4.5884e-02\n",
      " -1.4401e+00 -1.6349e-01  4.9361e-01  2.1410e-01 -7.0110e-01  2.3067e-01\n",
      " -1.1803e+00  6.5701e-02 -4.6429e-02  8.0979e-02 -1.6424e-01 -7.2896e-01\n",
      " -2.1221e-01  3.4235e-02 -4.0642e-01  2.8826e-01 -8.1331e-01 -6.7997e-02\n",
      " -2.5439e-01  1.3735e-01  1.0103e+00 -7.7614e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 3.0731e-01  2.4737e-01  6.8231e-01 -5.2367e-01  4.4053e-01  4.2044e-01\n",
      "  2.5140e-04  1.5265e-01 -6.1363e-01  2.2631e-01  8.3071e-02  7.0425e-02\n",
      "  1.7683e-02  5.6807e-01  1.0067e+00 -4.6206e-01  4.4524e-01 -5.0984e-01\n",
      " -4.2985e-01  1.9935e-01  2.2729e-01  5.1662e-01  5.6282e-01  4.1282e-01\n",
      "  1.7742e-01 -1.5694e-01 -1.1505e-01 -3.8050e-01  4.7440e-01 -1.6686e-01\n",
      "  2.3153e-01  6.3698e-02 -1.0716e-01 -2.6848e-01 -4.2665e-01  5.2237e-01\n",
      "  9.5376e-02  6.4020e-01 -5.2221e-01 -1.3856e-01 -9.8307e-01 -3.5320e-01\n",
      " -5.2161e-01  1.1277e-01  3.1634e-01  1.3297e-01 -4.9571e-02 -1.3785e-01\n",
      "  1.1317e-01 -5.0644e-01  3.8373e-01  3.6698e-01  3.9106e-01  9.8143e-01\n",
      " -5.4410e-01 -2.4640e+00 -6.8383e-01 -9.6243e-01  2.2017e+00  5.6643e-01\n",
      " -4.9410e-02  1.3093e+00 -4.0073e-01  8.3530e-01  1.7440e-01  4.4926e-02\n",
      "  5.4118e-01 -1.1038e-01  3.8200e-01  1.5369e-01 -3.7072e-01 -1.3141e-01\n",
      " -5.2504e-01 -5.6775e-01 -1.6822e-01 -9.1726e-02  8.1418e-02  4.5884e-02\n",
      " -1.4401e+00 -1.6349e-01  4.9361e-01  2.1410e-01 -7.0110e-01  2.3067e-01\n",
      " -1.1803e+00  6.5701e-02 -4.6429e-02  8.0979e-02 -1.6424e-01 -7.2896e-01\n",
      " -2.1221e-01  3.4235e-02 -4.0642e-01  2.8826e-01 -8.1331e-01 -6.7997e-02\n",
      " -2.5439e-01  1.3735e-01  1.0103e+00 -7.7614e-01]\n"
     ]
    }
   ],
   "source": [
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reset_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-f36541818602>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#    the movie review data in this assignment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# --------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mreset_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'reset_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6c6969396e8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbasic_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-c8674974ede7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = keras.Sequential([\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEVOCABSIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(32, kernel_size=3, activation=  'relu', input_shape=(40,50)),\n",
    "  keras.layers.GlobalAveragePooling1D(),\n",
    "  keras.layers.Dense(16, activation='relu'),\n",
    "  keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
